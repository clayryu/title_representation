
  0%|                                                                                                                 | 0/9000 [00:00<?, ?it/s]
0.0004
  0%|                                                                                                                    | 0/1 [00:00<?, ?it/s]

An arbitrary loader is used instead of Validation loader

  0%|                                                                                                      | 1/9000 [00:14<36:59:34, 14.80s/it]


  0%|                                                                                                      | 3/9000 [00:27<20:37:43,  8.25s/it]
0.0004


  0%|                                                                                                      | 5/9000 [00:40<17:39:52,  7.07s/it]
0.0004

  0%|                                                                                                      | 6/9000 [00:46<17:08:45,  6.86s/it]


  0%|                                                                                                      | 8/9000 [00:59<16:51:16,  6.75s/it]
  0%|                                                                                                      | 8/9000 [01:02<19:30:51,  7.81s/it]
Traceback (most recent call last):
  File "emb_train.py", line 167, in <module>
    trainer.train_by_num_epoch(args.num_epochs)
  File "/home/clay/userdata/title_generation/emb_trainer.py", line 80, in train_by_num_epoch
    for batch in self.train_loader:
  File "/home/clay/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 681, in __next__
    data = self._next_data()
  File "/home/clay/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 721, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/clay/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/clay/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/clay/.local/lib/python3.8/site-packages/torch/utils/data/dataset.py", line 290, in __getitem__
    return self.dataset[self.indices[idx]]
  File "/home/clay/userdata/title_generation/emb_data_utils.py", line 573, in __getitem__
    tune_in_idx = [self.vocab(token, new_header) for token in tune]
  File "/home/clay/userdata/title_generation/emb_data_utils.py", line 573, in <listcomp>
    tune_in_idx = [self.vocab(token, new_header) for token in tune]
  File "/home/clay/userdata/title_generation/vocab_utils.py", line 356, in __call__
    return self(wrd) + self.encode_m_idx(m_idx) + self.encode_m_offset(m_offset, header)
  File "/home/clay/userdata/title_generation/vocab_utils.py", line 310, in encode_m_offset
    middle_beat = [x * unit_beat for x in middle_beat]
KeyboardInterrupt